{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A set of functions that can be used to compare Randomized Response to RAPPOR\n",
    "\n",
    "This library contains functions that can generate random PMFs, draw samples from\n",
    "a given PMF, enocde data using both RAPPOR and Randomized Response, and apply\n",
    "various decoders for both RAPPOR and Randomized Response.\n",
    "\"\"\"\n",
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "def project_probability_simplex(p_estimate):\n",
    "  \"\"\"Projects a PMF estimate onto the probability simplex.\n",
    "\n",
    "  Args:\n",
    "    pmf_estimate: A PMF estimate.\n",
    "  This functions implements Algorithm 1 in http://arxiv.org/abs/1309.1541\n",
    "  \"\"\"\n",
    "  k = len(p_estimate)  # Infer the size of the alphabet.\n",
    "  p_estimate_sorted = np.sort(p_estimate)\n",
    "  p_estimate_sorted[:] = p_estimate_sorted[::-1]\n",
    "  p_sorted_cumsum = np.cumsum(p_estimate_sorted)\n",
    "  i = 1\n",
    "  while i < k:\n",
    "    if p_estimate_sorted[i] + (1.0 / (i + 1)) * (1 - p_sorted_cumsum[i]) < 0:\n",
    "      break\n",
    "    i += 1\n",
    "  lmd = (1.0 / i) * (1 - p_sorted_cumsum[i - 1])\n",
    "  return np.maximum(p_estimate + lmd, 0)\n",
    "\n",
    "\n",
    "#clip and normalize\n",
    "def probability_normalize(dist):\n",
    "    dist = np.maximum(dist,0) #map it to be positive\n",
    "    norm = np.sum(dist)\n",
    "    dist = np.true_divide(dist,norm) #ensure the l_1 norm is one\n",
    "    return dist\n",
    "\n",
    "def decode_counts(counts, epsilon, n, k):\n",
    "  \"\"\"Estimates PMF under RAPPOR and Randomized Response using standard decoding.\n",
    "\n",
    "  Args:\n",
    "    counts: An array containing the number of times each element showed up.\n",
    "    epsilon: The differential privacy level. If decode_counts is used for RAPPOR\n",
    "      epsilon/2 is passed to the function.\n",
    "    n: The number of samples (users).\n",
    "    k: The size of the alphabet. If decode_counts is used for RAPPOR k = 2 is\n",
    "      passed to the function.\n",
    "  \"\"\"\n",
    "  p_estimate = (counts / float(n)) * (\n",
    "      (math.exp(epsilon) + k - 1) /\n",
    "      (math.exp(epsilon) - 1)) - 1.0 / (math.exp(epsilon) - 1)\n",
    "  return p_estimate\n",
    "\n",
    "def randomized_response_encoder(samples, epsilon, k):\n",
    "  \"\"\"Encodes the samples using Randomized Response.\n",
    "\n",
    "  Args:\n",
    "    samples: The samples to be encoded.\n",
    "    epsilon: The differential privacy level.\n",
    "    k: The size of the alphabet.\n",
    "  \"\"\"\n",
    "  n = len(samples)\n",
    "  q = math.exp(epsilon) / (math.exp(epsilon) + k - 1)\n",
    "  # Start by setting private_samples = samples.\n",
    "  private_samples_rr = np.copy(samples)\n",
    "  # Determine which samples need to be noised (\"flipped\").\n",
    "  flip = np.random.random_sample(n) > q\n",
    "  flip_samples = samples[flip]\n",
    "  # Select new samples uniformly at random to replace the original ones.\n",
    "  rand_samples = np.random.randint(0, k - 1, len(flip_samples))\n",
    "  # Shift the samples if needed to avoid sampling the orginal samples.\n",
    "  rand_samples[rand_samples >= flip_samples] += 1\n",
    "  # Replace the original samples by the randomly selected ones.\n",
    "  private_samples_rr[flip] = rand_samples\n",
    "  return private_samples_rr\n",
    "\n",
    "\n",
    "def rappor_encoder(samples, epsilon, k):\n",
    "  \"\"\"Encodes the samples using RAPPOR.\n",
    "\n",
    "  Args:\n",
    "    samples: A 1-d numpy array of length n, where each entry is an integer in\n",
    "      [0,k). These are the input values to be RAPPOR-encoded.\n",
    "    epsilon: The differential privacy level.\n",
    "    k: The size of the alphabet.\n",
    "  \"\"\"\n",
    "  n = len(samples)\n",
    "  users = range(n)\n",
    "  q = math.exp(epsilon / 2.0) / (math.exp(epsilon / 2.0) + 1)\n",
    "  # One-hot encode the input integers.\n",
    "  private_samples_rappor = np.zeros((n, k))\n",
    "  private_samples_rappor[users, samples] = 1\n",
    "  # Flip the RAPPOR encoded bits with probability p = 1 - q.\n",
    "  flip = np.random.random_sample((n, k))\n",
    "  return np.logical_xor(private_samples_rappor, flip > q)\n",
    "\n",
    "\n",
    "def rappor_encoder_light(samples, epsilon, k):\n",
    "  #return to count vector of rappor responce, which is less memory intensive\n",
    "  #also return the cumulated time for adding rappor vectors, which should also be considered as decoding time.\n",
    "  n = len(samples)\n",
    "  users = range(n)\n",
    "  q = math.exp(epsilon / 2.0) / (math.exp(epsilon / 2.0) + 1)\n",
    "  time = 0\n",
    "  counts = np.zeros(k)\n",
    "  # One-hot encode the input integers.\n",
    "  for i in range(n):\n",
    "      private_samples_rappor = np.zeros(k)\n",
    "      private_samples_rappor[samples[i]] = 1\n",
    "      # Flip the RAPPOR encoded bits with probability p = 1 - q.\n",
    "      flip = np.random.random_sample(k)\n",
    "      private_samples_rappor = np.logical_xor(private_samples_rappor, flip > q) \n",
    "      start_time = timeit.default_timer() #record adding time\n",
    "      counts = counts + private_samples_rappor # add rappor responce vector\n",
    "      time = time + timeit.default_timer() - start_time      \n",
    "  return counts,time\n",
    "\n",
    "def rappor_encoder_compress(samples, epsilon, k):\n",
    "  #encode rappor responces into locations of one, which saves communcation budget when eps is large\n",
    "  n = len(samples)\n",
    "  users = range(n)\n",
    "  q = math.exp(epsilon / 2.0) / (math.exp(epsilon / 2.0) + 1)\n",
    "  out = [0]*n\n",
    "  # One-hot encode the input integers.\n",
    "  for i in range(n):\n",
    "      private_samples_rappor = np.zeros(k)\n",
    "      private_samples_rappor[samples[i]] = 1\n",
    "      # Flip the RAPPOR encoded bits with probability p = 1 - q.\n",
    "      flip = np.random.random_sample(k)\n",
    "      private_samples_rappor = np.logical_xor(private_samples_rappor, flip > q) \n",
    "      out[i] = np.where(private_samples_rappor)[0] # get the locations of ones\n",
    "  out_list = np.concatenate(out)\n",
    "  return out_list\n",
    "\n",
    "\n",
    "def rappor_decoder(counts_rappor, epsilon, n, normalization = 0):\n",
    "  \"\"\"Decodes RAPPOR encoded samples using a standard decoder.\n",
    "  \n",
    "  Args:\n",
    "    counts_rappor: A 1-d numpy array containing the counts under RAPPOR.\n",
    "    epsilon: The differential privacy level.\n",
    "    n: The number of samples.\n",
    "  \"\"\"\n",
    "  #normalization options: 0: clip and normalize(default)\n",
    "  #                       1: simplex projection\n",
    "  #                       else: no nomalization\n",
    "  # Estimate the PMF using the count vector\n",
    "  p_rappor = decode_counts(counts_rappor, epsilon / 2.0, n, 2)\n",
    "    \n",
    "  if normalization == 0: \n",
    "      p_rappor = probability_normalize(p_rappor) #clip and normalize\n",
    "  if normalization == 1:\n",
    "      p_rappor = project_probability_simplex(p_rappor) #simplex projection\n",
    "            \n",
    "\n",
    "  return p_rappor\n",
    "\n",
    "\n",
    "def rr_decoder(counts_rr, epsilon, n, normalization = 0):\n",
    "  \"\"\"Decodes RR encoded samples using a normalized standard decoder.\n",
    "\n",
    "  Args:\n",
    "    counts_rr: A 1-d numpy array containing the counts under RR.\n",
    "    epsilon: The differential privacy level.\n",
    "    n: The number of samples.\n",
    "  \"\"\"\n",
    "  #normalization options: 0: clip and normalize(default)\n",
    "  #                       1: simplex projection\n",
    "  #                       else: no nomalization\n",
    "  # Estimate the PMF using the count vector\n",
    "\n",
    "  k = len(counts_rr)  # Infer the size of the alphabet.\n",
    "  # Estimate the PMF using the count vector.\n",
    "  p_rr = decode_counts(counts_rr, epsilon, n, k)\n",
    "  # Check if truncation and renormalization is required.\n",
    "  \n",
    "  if normalization == 0: \n",
    "      p_rr = probability_normalize(p_rr) #clip and normalize\n",
    "  if normalization == 1:\n",
    "      p_rr = project_probability_simplex(p_rr) #simplex projection\n",
    "\n",
    "  return p_rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decode_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e7e61c523895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0moutp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#print outp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprob_est\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrr_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# estimate the original underlying distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"l1 distance: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma_i\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb_i\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_est\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"prob_sum: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_est\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-61c39bbe9396>\u001b[0m in \u001b[0;36mrr_decoder\u001b[0;34m(counts_rr, epsilon, n, normalization)\u001b[0m\n\u001b[1;32m    162\u001b[0m   \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts_rr\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Infer the size of the alphabet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# Estimate the PMF using the count vector.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   \u001b[0mp_rr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts_rr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m   \u001b[0;31m# Check if truncation and renormalization is required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decode_counts' is not defined"
     ]
    }
   ],
   "source": [
    "#Testing Script\n",
    "if __name__ == \"__main__\":\n",
    "    k = 500 #absz\n",
    "    n = 100000\n",
    "    elements = range(0,k) #ab\n",
    "    lbd = 0.8 #parameter for geometric dist\n",
    "    eps = 1 # privacy_para\n",
    "    prob = [(1-lbd)*math.pow(lbd,x)/(1-math.pow(lbd,k)) for x in elements] # geometric dist\n",
    "    #prob = [1/float(k)] * k\n",
    "    in_list = np.random.choice(elements, n, p=prob) #input symbols\n",
    "    sample = randomized_response_encoder(in_list, eps, k)\n",
    "    (outp, temp) = np.histogram(sample,range(k+1))\n",
    "    #print outp\n",
    "    prob_est = rr_decoder(outp,eps,n) # estimate the original underlying distribution\n",
    "    print (\"l1 distance: \", str(np.linalg.norm([a_i - b_i for a_i, b_i in zip(prob, prob_est)], ord=1)))\n",
    "    print (\"prob_sum: \", str(sum(prob_est)))\n",
    "    prob_est = rr_decoder(outp,eps,n,1) # estimate the original underlying distribution\n",
    "    plt.plot(elements,prob)\n",
    "    plt.plot(elements,prob_est)\n",
    "    print (\"l1 distance: \", str(np.linalg.norm([a_i - b_i for a_i, b_i in zip(prob, prob_est)], ord=1)))\n",
    "    print (\"prob_sum: \", str(sum(prob_est)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Script\n",
    "if __name__ == \"__main__\":\n",
    "    #sample = rappor_encoder(in_list, eps, k)\n",
    "    #outp = np.sum(sample, axis=0)\n",
    "    out_list = rappor_encoder_compress(in_list, eps, k)\n",
    "    outp,temp = np.histogram(out_list,range(k+1))\n",
    "    prob_est = rappor_decoder(outp,eps,n) # estimate the original underlying distribution\n",
    "    print (\"l1 distance: \", str(np.linalg.norm([a_i - b_i for a_i, b_i in zip(prob, prob_est)], ord=1)))\n",
    "    print (\"prob_sum: \", str(sum(prob_est)))\n",
    "    prob_est = rappor_decoder(outp,eps,n,1)\n",
    "    plt.plot(elements,prob)\n",
    "    plt.plot(elements,prob_est)\n",
    "    #plt.plot(prob_est)\n",
    "    print (\"l1 distance: \", str(np.linalg.norm([a_i - b_i for a_i, b_i in zip(prob, prob_est)], ord=1)))\n",
    "    print (\"prob_sum: \", str(sum(prob_est)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
