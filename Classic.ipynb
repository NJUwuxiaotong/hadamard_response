{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"A set of functions that can be used to compare Randomized Response to RAPPOR\n",
    "\n",
    "This library contains functions that can generate random PMFs, draw samples from\n",
    "a given PMF, enocde data using both RAPPOR and Randomized Response, and apply\n",
    "various decoders for both RAPPOR and Randomized Response.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "def ml_decoder(counts, n):\n",
    "  \"\"\"Implements the maximum likelihood estimator (ML) of a distribution.\n",
    "\n",
    "  Args:\n",
    "    counts: A 1-d array containing the number of times each element showed up.\n",
    "    n: The number of samples.\n",
    "  \"\"\"\n",
    "  return counts / float(n)\n",
    "\n",
    "\n",
    "def map_decoder(counts, params, n):\n",
    "  \"\"\"Implements the MAP estimator of a distribution under a Dirichlet prior.\n",
    "\n",
    "  Args:\n",
    "    counts: A 1-d array containing the number of times each element showed up.\n",
    "    params: A 1-d numpy array containing the parameters of a Dirichlet\n",
    "      distribution.\n",
    "    n: The number of samples.\n",
    "  \"\"\"\n",
    "  return (counts + params) / float(n + sum(params))\n",
    "\n",
    "\n",
    "def decode_counts(counts, epsilon, n, k):\n",
    "  \"\"\"Estimates PMF under RAPPOR and Randomized Response using standard decoding.\n",
    "\n",
    "  Args:\n",
    "    counts: An array containing the number of times each element showed up.\n",
    "    epsilon: The differential privacy level. If decode_counts is used for RAPPOR\n",
    "      epsilon/2 is passed to the function.\n",
    "    n: The number of samples (users).\n",
    "    k: The size of the alphabet. If decode_counts is used for RAPPOR k = 2 is\n",
    "      passed to the function.\n",
    "  \"\"\"\n",
    "  p_estimate = (counts / float(n)) * (\n",
    "      (math.exp(epsilon) + k - 1) /\n",
    "      (math.exp(epsilon) - 1)) - 1.0 / (math.exp(epsilon) - 1)\n",
    "  return p_estimate\n",
    "\n",
    "\n",
    "def truncate_and_normalize(p_estimate):\n",
    "  \"\"\"Truncates an estimated PMF and renormalizes it to sum to 1.\n",
    "\n",
    "  Args:\n",
    "    pmf_estimate: An estimate for the PMF.\n",
    "  \"\"\"\n",
    "  p_estimate_rectified = np.maximum(p_estimate, 0)\n",
    "  return p_estimate_rectified / sum(p_estimate_rectified)\n",
    "\n",
    "\n",
    "def sample_dirichlet(params):\n",
    "  \"\"\"Samples a PMF from the probability simplex according to a Dirichlet dist.\n",
    "\n",
    "  Args:\n",
    "    params: A 1-d numpy array containing the parameters of a Dirichlet dist.\n",
    "  \"\"\"\n",
    "  pmf = np.array([random.gammavariate(a, 1) for a in params])\n",
    "  pmf = pmf / sum(pmf)\n",
    "  return pmf\n",
    "\n",
    "\n",
    "def discrete_pmf_samples(pmf, n):\n",
    "  \"\"\"Generates n samples from a given PMF.\n",
    "\n",
    "  Args:\n",
    "    pmf: A given PMF to be sampled from.\n",
    "    n: The number of samples to be generated.\n",
    "  \"\"\"\n",
    "  bins = np.cumsum(pmf)\n",
    "  # Sample n samples from the alphabet according to the given PMF.\n",
    "  samples = np.digitize(np.random.random_sample(n), bins)\n",
    "  return samples\n",
    "\n",
    "\n",
    "def randomized_response_encoder(samples, epsilon, k):\n",
    "  \"\"\"Encodes the samples using Randomized Response.\n",
    "\n",
    "  Args:\n",
    "    samples: The samples to be encoded.\n",
    "    epsilon: The differential privacy level.\n",
    "    k: The size of the alphabet.\n",
    "  \"\"\"\n",
    "  n = len(samples)\n",
    "  q = math.exp(epsilon) / (math.exp(epsilon) + k - 1)\n",
    "  # Start by setting private_samples = samples.\n",
    "  private_samples_rr = np.copy(samples)\n",
    "  # Determine which samples need to be noised (\"flipped\").\n",
    "  flip = np.random.random_sample(n) > q\n",
    "  flip_samples = samples[flip]\n",
    "  # Select new samples uniformly at random to replace the original ones.\n",
    "  rand_samples = np.random.randint(0, k - 1, len(flip_samples))\n",
    "  # Shift the samples if needed to avoid sampling the orginal samples.\n",
    "  rand_samples[rand_samples >= flip_samples] += 1\n",
    "  # Replace the original samples by the randomly selected ones.\n",
    "  private_samples_rr[flip] = rand_samples\n",
    "  return private_samples_rr\n",
    "\n",
    "\n",
    "def encode_rappor(counts, epsilon, n):\n",
    "  \"\"\"Creates the count vector under RAPPOR.\n",
    "\n",
    "  Args:\n",
    "    counts: A 1-d numpy array containing a random draw from\n",
    "      multinomial(n, original PMF).\n",
    "    epsilon: The differential privacy level.\n",
    "    n: The number of samples over which the count vector is to be computed.\n",
    "  This function implements the efficient RAPPOR encoding algorithm described in:\n",
    "  https://cs.corp.google.com/#piper///depot/google3/experimental/users/kairouz/Optimal_Decoding/private_distribution_estimation.pdf\n",
    "  \"\"\"\n",
    "  counts_rappor = np.zeros(len(counts))\n",
    "  q = math.exp(epsilon / 2.0) / (math.exp(epsilon / 2.0) + 1)\n",
    "  for (i, count) in enumerate(counts):\n",
    "    counts_rappor[i] = np.random.binomial(count,q) + np.random.binomial(\n",
    "        n - count, 1 - q)\n",
    "  return counts_rappor\n",
    "\n",
    "\n",
    "def encode_randomized_response(pmf, epsilon, n):\n",
    "  \"\"\"Creates the count vector under Randomized Response.\n",
    "\n",
    "  Args:\n",
    "    pmf: A 1-d numpy array containing the original PMF.\n",
    "    epsilon: The differential privacy level.\n",
    "    n: The number of samples over which the count vector is to be computed.\n",
    "  This function creates a random draw from multinomial(n, private PMF).\n",
    "  Let q = exp(epsilon) / (exp(epsilon) + k - 1),  then RR is given by\n",
    "    p(out | in) = q            if out == in\n",
    "                = (1-q)/(k-1)  otherwise,\n",
    "  and therefore, the private PMF is given by\n",
    "    private_pmf[i] = p(out=i)\n",
    "                   = sum_i p(out=i | in=i) p(in = i)\n",
    "                   = q * pmf[i] + (1-q)/(k-1) * (1-pmf[i])\n",
    "                   =  (exp(epsilon)* pmf[i] + 1 - pmf[i]) / (\n",
    "                          exp(epsilon) + k - 1)\n",
    "                   =  ( (exp(epsilon)-1)*pmf[i] + 1 ) / (\n",
    "                          exp(epsilon) + k - 1)\n",
    "  \"\"\"\n",
    "  # Compute M (the private PMF) = RR mechanism applied to P (the original PMF).\n",
    "  private_pmf = ((math.exp(epsilon) - 1) * pmf + 1) / (\n",
    "      math.exp(epsilon) + len(pmf) - 1)\n",
    "  return np.random.multinomial(n, private_pmf)\n",
    "\n",
    "\n",
    "def rappor_encoder(samples, epsilon, k):\n",
    "  \"\"\"Encodes the samples using RAPPOR.\n",
    "\n",
    "  Args:\n",
    "    samples: A 1-d numpy array of length n, where each entry is an integer in\n",
    "      [0,k). These are the input values to be RAPPOR-encoded.\n",
    "    epsilon: The differential privacy level.\n",
    "    k: The size of the alphabet.\n",
    "  \"\"\"\n",
    "  n = len(samples)\n",
    "  users = range(n)\n",
    "  q = math.exp(epsilon / 2.0) / (math.exp(epsilon / 2.0) + 1)\n",
    "  # One-hot encode the input integers.\n",
    "  private_samples_rappor = np.zeros((n, k))\n",
    "  private_samples_rappor[users, samples] = 1\n",
    "  # Flip the RAPPOR encoded bits with probability p = 1 - q.\n",
    "  flip = np.random.random_sample((n, k))\n",
    "  return np.logical_xor(private_samples_rappor, flip > q)\n",
    "\n",
    "def rappor_encoder_light(samples, epsilon, k):\n",
    "  n = len(samples)\n",
    "  users = range(n)\n",
    "  q = math.exp(epsilon / 2.0) / (math.exp(epsilon / 2.0) + 1)\n",
    "  time = 0\n",
    "  counts = np.zeros(k)\n",
    "  # One-hot encode the input integers.\n",
    "  for i in range(n):\n",
    "      private_samples_rappor = np.zeros(k)\n",
    "      private_samples_rappor[samples[i]] = 1\n",
    "      # Flip the RAPPOR encoded bits with probability p = 1 - q.\n",
    "      flip = np.random.random_sample(k)\n",
    "      private_samples_rappor = np.logical_xor(private_samples_rappor, flip > q) \n",
    "      start_time = timeit.default_timer()\n",
    "      counts = counts + private_samples_rappor\n",
    "      time = time + timeit.default_timer() - start_time      \n",
    "  return counts,time\n",
    "\n",
    "def project_probability_simplex(p_estimate):\n",
    "  \"\"\"Projects a PMF estimate onto the probability simplex.\n",
    "\n",
    "  Args:\n",
    "    pmf_estimate: A PMF estimate.\n",
    "  This functions implements Algorithm 1 in http://arxiv.org/abs/1309.1541\n",
    "  \"\"\"\n",
    "  k = len(p_estimate)  # Infer the size of the alphabet.\n",
    "  p_estimate_sorted = np.sort(p_estimate)\n",
    "  p_estimate_sorted[:] = p_estimate_sorted[::-1]\n",
    "  p_sorted_cumsum = np.cumsum(p_estimate_sorted)\n",
    "  i = 1\n",
    "  while i < k:\n",
    "    if p_estimate_sorted[i] + (1.0 / (i + 1)) * (1 - p_sorted_cumsum[i]) < 0:\n",
    "      break\n",
    "    i += 1\n",
    "  lmd = (1.0 / i) * (1 - p_sorted_cumsum[i - 1])\n",
    "  return np.maximum(p_estimate + lmd, 0)\n",
    "\n",
    "\n",
    "def standard_rappor_decoder(counts_rappor, epsilon, n):\n",
    "  \"\"\"Decodes RAPPOR encoded samples using a standard decoder.\n",
    "\n",
    "  Args:\n",
    "    counts_rappor: A 1-d numpy array containing the counts under RAPPOR.\n",
    "    epsilon: The differential privacy level.\n",
    "    n: The number of samples.\n",
    "  \"\"\"\n",
    "  # Estimate the PMF using the count vector\n",
    "  p_rappor = decode_counts(counts_rappor, epsilon / 2.0, n, 2)\n",
    "  return np.maximum(p_rappor, 0)\n",
    "\n",
    "\n",
    "def normalized_standard_rappor_decoder(counts_rappor, epsilon, n):\n",
    "  \"\"\"Decodes RAPPOR encoded samples using a normalized standard decoder.\n",
    "\n",
    "  Args:\n",
    "    counts_rappor: A 1-d numpy array containing the counts under RAPPOR.\n",
    "    epsilon: The differential privacy level.\n",
    "    n: The number of samples.\n",
    "  \"\"\"\n",
    "  # Estimate the PMF using the count vector.\n",
    "  p_rappor = decode_counts(counts_rappor, epsilon / 2.0, n, 2)\n",
    "  if np.amax(p_rappor) <= 0:\n",
    "    p_rappor_rectified = np.zeros(len(counts_rappor))\n",
    "  else:\n",
    "    # Truncate the negative entries and renormalize the PMF to sum to 1.\n",
    "    p_rappor_rectified = truncate_and_normalize(p_rappor)\n",
    "  return p_rappor_rectified\n",
    "\n",
    "\n",
    "def projection_rappor_decoder(counts_rappor, epsilon, n):\n",
    "  \"\"\"Decodes RAPPOR encoded samples using a projection decoder.\n",
    "\n",
    "  Args:\n",
    "    counts_rappor: A 1-d numpy array containing the counts under RAPPOR.\n",
    "    epsilon: The differential privacy level.\n",
    "    n: The number of samples.\n",
    "  \"\"\"\n",
    "  # Estimate the PMF using the count vector.\n",
    "  p_rappor = decode_counts(counts_rappor, epsilon / 2.0, n, 2)\n",
    "  # Project the PMF estimate onto the (k-1)-dimensional probability simplex.\n",
    "  return project_probability_simplex(p_rappor)\n",
    "\n",
    "\n",
    "def normalized_standard_rr_decoder(counts_rr, epsilon, n):\n",
    "  \"\"\"Decodes RR encoded samples using a normalized standard decoder.\n",
    "\n",
    "  Args:\n",
    "    counts_rr: A 1-d numpy array containing the counts under RR.\n",
    "    epsilon: The differential privacy level.\n",
    "    n: The number of samples.\n",
    "  \"\"\"\n",
    "  k = len(counts_rr)  # Infer the size of the alphabet.\n",
    "  # Estimate the PMF using the count vector.\n",
    "  p_rr = decode_counts(counts_rr, epsilon, n, k)\n",
    "  # Check if truncation and renormalization is required.\n",
    "  if np.amin(p_rr) < 0:\n",
    "    # Truncate the negative entries and renormalize the PMF to sum to 1.\n",
    "    p_rr_rectified = truncate_and_normalize(p_rr)\n",
    "  else:\n",
    "    p_rr_rectified = p_rr\n",
    "  assert abs(np.sum(p_rr_rectified) - 1) < 1e-3\n",
    "  return p_rr_rectified\n",
    "\n",
    "\n",
    "def projection_rr_decoder(counts_rr, epsilon, n):\n",
    "  \"\"\"Decodes RR encoded samples using a projection decoder.\n",
    "\n",
    "  Args:\n",
    "    counts_rr: A 1-d numpy array containing the counts under RR.\n",
    "    epsilon: The differential privacy level.\n",
    "    n: The number of samples.\n",
    "  \"\"\"\n",
    "  k = len(counts_rr)  # Infer the size of the alphabet.\n",
    "  # Estimate the PMF using the count vector.\n",
    "  p_rr = decode_counts(counts_rr, epsilon, n, k)\n",
    "  # Check if a projection onto the probability simplex is needed.\n",
    "  if np.amin(p_rr) < 0:\n",
    "    p_rr_rectified = project_probability_simplex(p_rr)\n",
    "  else:\n",
    "    p_rr_rectified = p_rr\n",
    "  return p_rr_rectified\n",
    "\n",
    "\n",
    "def maximum_likelihood_rr_decoder(counts_rr, epsilon, n):\n",
    "  \"\"\"Decodes RR encoded samples using a maximum likelihood decoder.\n",
    "\n",
    "  Args:\n",
    "    counts_rr: A 1-d numpy array containing the counts under RR.\n",
    "    epsilon: The differential privacy level.\n",
    "    n: The number of samples.\n",
    "  \"\"\"\n",
    "  k = len(counts_rr)  # Infer the size of the alphabet.\n",
    "  # Estimate the PMF using the count vector.\n",
    "  p_rr = decode_counts(counts_rr, epsilon, n, k)\n",
    "  # Check if water filling is required.\n",
    "  if np.amin(p_rr) < 0:\n",
    "    indices = np.argsort(counts_rr)\n",
    "    p_rr_rectified = np.zeros(k)\n",
    "    for i in xrange(k):\n",
    "      p_rr_rectified[i] = 0\n",
    "      normalized_sum = float(np.sum(counts_rr[indices[i + 1:]]))\n",
    "      p_rr_rectified[i + 1:] = (counts_rr[indices[i + 1:]] / normalized_sum) * (\n",
    "          1 + (k - i - 1.0) /\n",
    "          (math.exp(epsilon) - 1)) - 1.0 / (math.exp(epsilon) - 1)\n",
    "      if np.amin(p_rr_rectified) >= 0:\n",
    "        break\n",
    "    # Rearrange p_rr_rectified to follow the same sequence p_rr had.\n",
    "    reverse_indices = np.zeros(k, dtype=np.int)\n",
    "    reverse_indices[indices] = np.arange(k)\n",
    "    p_rr_rectified = p_rr_rectified[reverse_indices]\n",
    "  else:\n",
    "    p_rr_rectified = p_rr\n",
    "  return p_rr_rectified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l1 distance: ', '0.318770091908')\n",
      "('prob_sum: ', '1.0')\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    k = 100 #absz\n",
    "    n = 1000000\n",
    "    elements = range(0,k) #ab\n",
    "    lbd = 0.8 #parameter for geometric dist\n",
    "    eps = 1 # privacy_para\n",
    "    prob = [(1-lbd)*math.pow(lbd,x)/(1-math.pow(lbd,k)) for x in elements] # geometric dist\n",
    "\n",
    "    #prob = [1/float(k)] * k\n",
    "    in_list = np.random.choice(elements, n, p=prob) #input symbols\n",
    "    sample = randomized_response_encoder(in_list, eps, k)\n",
    "    (outp, temp) = np.histogram(sample,range(k+1))\n",
    "    #print outp\n",
    "    prob_est = normalized_standard_rr_decoder(outp,eps,n) # estimate the original underlying distribution\n",
    "    plt.plot(elements,prob)\n",
    "    plt.plot(elements,prob_est)\n",
    "    #plt.plot(prob_est)\n",
    "    print (\"l1 distance: \", str(np.linalg.norm([a_i - b_i for a_i, b_i in zip(prob, prob_est)], ord=1)))\n",
    "    print (\"prob_sum: \", str(sum(prob_est)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l1 distance: ', '0.122965424126')\n",
      "('prob_sum: ', '1.0')\n",
      "1.55851888657\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #sample = rappor_encoder(in_list, eps, k)\n",
    "    #outp = np.sum(sample, axis=0)\n",
    "    outp, time = rappor_encoder_light(in_list, eps, k)\n",
    "    prob_est = normalized_standard_rappor_decoder(outp,eps,n) # estimate the original underlying distribution\n",
    "    plt.plot(elements,prob)\n",
    "    plt.plot(elements,prob_est)\n",
    "    #plt.plot(prob_est)\n",
    "    print (\"l1 distance: \", str(np.linalg.norm([a_i - b_i for a_i, b_i in zip(prob, prob_est)], ord=1)))\n",
    "    print (\"prob_sum: \", str(sum(prob_est)))\n",
    "    print (time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
